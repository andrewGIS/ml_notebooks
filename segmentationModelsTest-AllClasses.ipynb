{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import segmentation_models as sm\n",
    "\n",
    "from tensorflow import keras\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "\n",
    "\n",
    "from segmentation_models import Unet\n",
    "from tensorflow.keras.layers import Input, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from skimage import io\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import imgaug as ia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such model `None`, available models: ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnet152', 'seresnext50', 'seresnext101', 'senet154', 'resnext50', 'resnext101', 'vgg16', 'vgg19', 'densenet121', 'densenet169', 'densenet201', 'inceptionresnetv2', 'inceptionv3', 'mobilenet', 'mobilenetv2', 'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5', 'efficientnetb6', 'efficientnetb7']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bde366df591c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_last/lib/python3.8/site-packages/segmentation_models/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_KERAS_MODELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_KERAS_UTILS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_last/lib/python3.8/site-packages/segmentation_models/models/unet.py\u001b[0m in \u001b[0;36mUnet\u001b[0;34m(backbone_name, input_shape, classes, activation, weights, encoder_weights, encoder_freeze, encoder_features, decoder_block_type, decoder_filters, decoder_use_batchnorm, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m                          'Got: {}'.format(decoder_block_type))\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     backbone = Backbones.get_backbone(\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mbackbone_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_last/lib/python3.8/site-packages/segmentation_models/backbones/backbones_factory.py\u001b[0m in \u001b[0;36mget_backbone\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_last/lib/python3.8/site-packages/classification_models/models_factory.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             raise ValueError('No such model `{}`, available models: {}'.format(\n\u001b[0m\u001b[1;32m     85\u001b[0m                 name, list(self.models_names())))\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such model `None`, available models: ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnet152', 'seresnext50', 'seresnext101', 'senet154', 'resnext50', 'resnext101', 'vgg16', 'vgg19', 'densenet121', 'densenet169', 'densenet201', 'inceptionresnetv2', 'inceptionv3', 'mobilenet', 'mobilenetv2', 'efficientnetb0', 'efficientnetb1', 'efficientnetb2', 'efficientnetb3', 'efficientnetb4', 'efficientnetb5', 'efficientnetb6', 'efficientnetb7']"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACKBONE = 'vgg16'\n",
    "BATCH_SIZE = 8\n",
    "#CLASSES = ['car']\n",
    "#LR = 0.001\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BACKBONE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1a275ccdf539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BACKBONE' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BACKBONE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bb634b316c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BACKBONE' is not defined"
     ]
    }
   ],
   "source": [
    "# define network parameters\n",
    "n_classes = 2  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "#create model\n",
    "model = sm.Unet(BACKBONE, classes=2, activation='softmax',input_shape=(256,256,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del base_model\n",
    "del  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Unet(\n",
    "    backbone_name='vgg16', \n",
    "    encoder_weights='./files/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    activation='softmax',\n",
    "    classes=3\n",
    ")\n",
    "\n",
    "inp = Input(shape=(256, 256, 16))\n",
    "l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n",
    "out = base_model(l1)\n",
    "\n",
    "model = Model(inp, out, name=base_model.name)\n",
    "\n",
    "# small update 29.11.2020 Misha has noticed that it is possible to \n",
    "# set input shape in model creation too\n",
    "#model = Unet(\n",
    "#    backbone_name='vgg16', \n",
    "#    encoder_weights=None,\n",
    "#    activation='sigmoid',\n",
    "#    input_shape=(256, 256, 5)\n",
    "#)\n",
    "\n",
    "# continue with usual steps: compile, fit, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LR Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-5\n",
    "    if epoch == 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch == 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch == 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch == 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = keras.callbacks.ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = keras.optimizers.Adam(learning_rate=lr_schedule(0))\n",
    "optim = keras.optimizers.Adam()\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "#dice_loss = sm.losses.DiceLoss()\n",
    "#focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "#total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(optim, focal_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder where dates is intermidiate in season\n",
    "# with show -> without snow\n",
    "# without show -> with snow\n",
    "\"\"\"flds = [\n",
    "    'T39VVJ_pair_0.tif',\n",
    "    'T39VWH_pair_2.tif',\n",
    "    'T39VVJ_pair_4.tif',\n",
    "    'T40VEM_pair_0.tif',\n",
    "    'T39VWJ_pair_5.tif',\n",
    "    'T39VXJ_pair_3.tif',\n",
    "    'T39VWJ_pair_3.tif',\n",
    "    'T39VXH_pair_5.tif',\n",
    "    'T39VXH_pair_3.tif',\n",
    "    'T39VXJ_pair_2.tif',\n",
    "    'T39VXJ_pair_5.tif',\n",
    "    'T40VDJ_pair_1.tif',\n",
    "    'T40VEL_pair_4.tif',\n",
    "    'T40VEL_pair_1.tif',\n",
    "    'T40VDJ_pair_4.tif',\n",
    "    'T39VWH_pair_4.tif',\n",
    "    'T40VEM_pair_2.tif',\n",
    "    'T40VFK_pair_8.tif',\n",
    "    'T40VFK_pair_3.tif',\n",
    "    'T39VWH_pair_8.tif',\n",
    "    'T41VLE_pair_3.tif',\n",
    "    'T40VFK_pair_5.tif',\n",
    "    'T41VLE_pair_1.tif',\n",
    "    'T41VLE_pair_2.tif'\n",
    "]\"\"\"\n",
    "\n",
    "# transition period filtered (with high and middle confidence)\n",
    "flds = [\n",
    "    'T39VVJ_pair_0.tif',\n",
    "    'T39VWH_pair_2.tif',\n",
    "    'T40VEM_pair_0.tif',\n",
    "    'T39VXJ_pair_3.tif',\n",
    "    'T39VXH_pair_5.tif',\n",
    "    'T39VXH_pair_3.tif',\n",
    "    'T39VXJ_pair_2.tif',\n",
    "    'T39VXJ_pair_5.tif',\n",
    "    'T40VDJ_pair_1.tif',\n",
    "    'T40VEL_pair_1.tif',\n",
    "    'T40VDJ_pair_4.tif',\n",
    "    'T39VWH_pair_4.tif',\n",
    "    'T40VEM_pair_2.tif',\n",
    "    'T39VWH_pair_8.tif',\n",
    "    'T41VLE_pair_3.tif',\n",
    "    'T40VFK_pair_5.tif',\n",
    "    'T41VLE_pair_1.tif',\n",
    "    'T41VLE_pair_2.tif',\n",
    "]\n",
    "\n",
    "\n",
    "#summer folders\n",
    "#without snow -> without snow\n",
    "'''flds = [\n",
    "     'T39VVJ_pair_1.tif',\n",
    "     'T39VVJ_pair_2.tif',\n",
    "     'T39VVJ_pair_3.tif',\n",
    "     'T39VWH_pair_3.tif',\n",
    "     'T39VXH_pair_0.tif',\n",
    "     'T39VWJ_pair_0.tif',\n",
    "     'T39VWH_pair_6.tif',\n",
    "     'T39VWH_pair_7.tif',\n",
    "     'T39VWJ_pair_4.tif',\n",
    "     'T39VWJ_pair_7.tif',\n",
    "     'T39VWJ_pair_8.tif',\n",
    "     'T39VXH_pair_4.tif',\n",
    "     'T39VXH_pair_6.tif',\n",
    "     'T39VXH_pair_7.tif',\n",
    "     'T39VXJ_pair_6.tif',\n",
    "     'T40VDJ_pair_2.tif',\n",
    "     'T40VDJ_pair_3.tif',\n",
    "     'T40VEM_pair_4.tif',\n",
    "     'T40VEL_pair_0.tif',\n",
    "     'T40VEM_pair_3.tif',\n",
    "     'T40VFK_pair_1.tif',\n",
    "     'T40VFK_pair_2.tif',\n",
    "     'T40VFK_pair_6.tif',\n",
    "     'T40VFK_pair_7.tif',\n",
    "     'T41VLE_pair_0.tif',\n",
    "]'''\n",
    "\n",
    "# summer changes filtered (with high confidence)\n",
    "'''flds = [\n",
    "    'T39VWH_pair_3.tif',\n",
    "    'T39VXH_pair_0.tif',\n",
    "    'T39VWH_pair_7.tif',\n",
    "    'T39VXH_pair_6.tif',\n",
    "    'T39VXH_pair_7.tif',\n",
    "    'T39VXJ_pair_6.tif',\n",
    "    'T40VDJ_pair_2.tif',\n",
    "    'T40VDJ_pair_3.tif',\n",
    "    'T40VEM_pair_4.tif',\n",
    "    'T41VLE_pair_0.tif',\n",
    "]'''\n",
    "\n",
    "\n",
    "\n",
    "#winter changes\n",
    "#with snow -> with snow\n",
    "\"\"\"flds = [\n",
    "     'T40VEK_pair_0.tif',\n",
    "     'T40VEK_pair_1.tif',\n",
    "     'T40VEK_pair_2.tif',\n",
    "     'T40VEK_pair_4.tif',\n",
    "     'T40VEM_pair_1.tif',\n",
    "     'T40VEK_pair_3.tif',\n",
    "     'T40VDJ_pair_0.tif',\n",
    "     'T40VFK_pair_4.tif',\n",
    "     'T39VXJ_pair_0.tif',\n",
    "     'T39VWJ_pair_6.tif',\n",
    "     'T39VXH_pair_1.tif',\n",
    "     'T40VFK_pair_0.tif',\n",
    "     'T40VEM_pair_5.tif',\n",
    "     'T39VWH_pair_0.tif',\n",
    "     'T39VXJ_pair_1.tif',\n",
    "     'T39VXH_pair_2.tif',\n",
    "     'T39VWJ_pair_1.tif',\n",
    "     'T40VEL_pair_2.tif',\n",
    "     'T40VEM_pair_6.tif',\n",
    "     'T40VEL_pair_3.tif',\n",
    "     'T39VWH_pair_1.tif',\n",
    "     'T39VXJ_pair_4.tif',\n",
    "     'T39VWJ_pair_2.tif',\n",
    "     'T40VEL_pair_5.tif',\n",
    "     'T39VWH_pair_5.tif',\n",
    "     'T40VEL_pair_6.tif',\n",
    "]\"\"\"\n",
    "\n",
    "\n",
    "#get all tifs from train folder\n",
    "#allTifs = list(glob.glob('../../ds/tiles_256_256_27_add/*/*.tif'))\n",
    "\n",
    "# with object tiffs\n",
    "#allTifs = list(glob.glob('../../ds/tiles_256_256_27/*/*with_object.tif'))\n",
    "\n",
    "# get all tiffs from defined list\n",
    "#allTifs = []\n",
    "#for fldName in flds:\n",
    "#    allTifs.extend(list(glob.glob(f'../../ds/tiles_256_256_27/{fldName}/*.tif')))\n",
    "\n",
    "#add tiles + checked tiles \n",
    "# delete control data\n",
    "notControl = ['T40VDN', 'T40VEN', 'T39VUH', 'T39VUK']\n",
    "addTiles = []\n",
    "for tile in list(glob.glob(f'../../ds/tiles_256_256_27_add/*/*.tif')):\n",
    "    tileName = os.path.basename(tile)\n",
    "    #print(tileName[:6])\n",
    "    if tileName[:6] in notControl:\n",
    "        addTiles.append(tile)\n",
    "allTifs = (\n",
    "    list(glob.glob(f'../../ds/tiles_256_256_27_after_check/*/*.tif'))\n",
    "    + addTiles\n",
    ")\n",
    "\n",
    "#shuffle\n",
    "np.random.shuffle(allTifs)\n",
    "\n",
    "#validate 15%\n",
    "#validate by hist by month\n",
    "validSize = int(len(allTifs)*0.15)\n",
    "    \n",
    "train_generator = DataGenerator(allTifs[:-validSize],batch_size=BATCH_SIZE)\n",
    "valid_generator = DataGenerator(allTifs[-validSize:],batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('./files/BackBoneTest.h5', \n",
    "                                    save_weights_only=True, \n",
    "                                    save_best_only=True, \n",
    "                                    mode='min'),\n",
    "    #keras.callbacks.ReduceLROnPlateau(),\n",
    "    #lr_scheduler,\n",
    "    lr_reducer\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "357/357 [==============================] - 318s 890ms/step - loss: 2.9296e-05 - iou_score: 0.3700 - f1-score: 0.3729 - val_loss: 2.6654e-05 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "357/357 [==============================] - 320s 895ms/step - loss: 1.5811e-05 - iou_score: 0.3456 - f1-score: 0.3484 - val_loss: 1.0488e-05 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "357/357 [==============================] - 321s 899ms/step - loss: 1.2616e-05 - iou_score: 0.4007 - f1-score: 0.4035 - val_loss: 9.3142e-06 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "357/357 [==============================] - 317s 888ms/step - loss: 1.2192e-05 - iou_score: 0.4978 - f1-score: 0.5006 - val_loss: 1.3151e-05 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "357/357 [==============================] - 318s 891ms/step - loss: 1.2620e-05 - iou_score: 0.5407 - f1-score: 0.5435 - val_loss: 1.2131e-05 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "357/357 [==============================] - 315s 883ms/step - loss: 1.1712e-05 - iou_score: 0.5155 - f1-score: 0.5183 - val_loss: 3.7869e-05 - val_iou_score: 0.6278 - val_f1-score: 0.6311 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "357/357 [==============================] - 317s 887ms/step - loss: 1.1253e-05 - iou_score: 0.5323 - f1-score: 0.5351 - val_loss: 8.9690e-06 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 3.1623e-04\n",
      "Epoch 8/50\n",
      "357/357 [==============================] - 313s 876ms/step - loss: 1.1308e-05 - iou_score: 0.5949 - f1-score: 0.5977 - val_loss: 2.1649e-05 - val_iou_score: 0.3368 - val_f1-score: 0.3401 - lr: 3.1623e-04\n",
      "Epoch 9/50\n",
      "357/357 [==============================] - 313s 877ms/step - loss: 1.0178e-05 - iou_score: 0.5034 - f1-score: 0.5062 - val_loss: 8.8419e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-04\n",
      "Epoch 10/50\n",
      "357/357 [==============================] - 315s 881ms/step - loss: 1.1491e-05 - iou_score: 0.4810 - f1-score: 0.4838 - val_loss: 9.8364e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-04\n",
      "Epoch 11/50\n",
      "357/357 [==============================] - 324s 908ms/step - loss: 1.1151e-05 - iou_score: 0.5827 - f1-score: 0.5855 - val_loss: 8.6016e-06 - val_iou_score: 0.5749 - val_f1-score: 0.5782 - lr: 3.1623e-04\n",
      "Epoch 12/50\n",
      "357/357 [==============================] - 316s 884ms/step - loss: 1.0154e-05 - iou_score: 0.6070 - f1-score: 0.6098 - val_loss: 8.4675e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "357/357 [==============================] - 315s 882ms/step - loss: 1.0213e-05 - iou_score: 0.6537 - f1-score: 0.6565 - val_loss: 8.4239e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "357/357 [==============================] - 315s 882ms/step - loss: 1.0254e-05 - iou_score: 0.6565 - f1-score: 0.6593 - val_loss: 8.3073e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "357/357 [==============================] - 313s 876ms/step - loss: 1.0075e-05 - iou_score: 0.6285 - f1-score: 0.6313 - val_loss: 1.2944e-05 - val_iou_score: 0.5696 - val_f1-score: 0.5729 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "357/357 [==============================] - 312s 874ms/step - loss: 9.7905e-06 - iou_score: 0.6294 - f1-score: 0.6322 - val_loss: 1.7805e-05 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "357/357 [==============================] - 315s 882ms/step - loss: 9.7996e-06 - iou_score: 0.6584 - f1-score: 0.6612 - val_loss: 8.1493e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-05\n",
      "Epoch 18/50\n",
      "357/357 [==============================] - 317s 888ms/step - loss: 9.6740e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.9368e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-05\n",
      "Epoch 19/50\n",
      "357/357 [==============================] - 309s 866ms/step - loss: 1.0202e-05 - iou_score: 0.6583 - f1-score: 0.6611 - val_loss: 9.8263e-06 - val_iou_score: 0.6542 - val_f1-score: 0.6576 - lr: 3.1623e-05\n",
      "Epoch 20/50\n",
      "357/357 [==============================] - 315s 882ms/step - loss: 9.7317e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.2584e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-05\n",
      "Epoch 21/50\n",
      "357/357 [==============================] - 316s 885ms/step - loss: 9.4742e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 7.9706e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-05\n",
      "Epoch 22/50\n",
      "357/357 [==============================] - 315s 883ms/step - loss: 9.2762e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0556e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "357/357 [==============================] - 316s 884ms/step - loss: 9.3412e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1750e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "357/357 [==============================] - 317s 887ms/step - loss: 9.4514e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1551e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "357/357 [==============================] - 319s 892ms/step - loss: 9.3427e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0081e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "357/357 [==============================] - 316s 884ms/step - loss: 9.5006e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0574e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "357/357 [==============================] - 321s 899ms/step - loss: 9.3795e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0692e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-06\n",
      "Epoch 28/50\n",
      "357/357 [==============================] - 315s 883ms/step - loss: 9.5708e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.2778e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-06\n",
      "Epoch 29/50\n",
      "357/357 [==============================] - 318s 891ms/step - loss: 9.2338e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0895e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-06\n",
      "Epoch 30/50\n",
      "357/357 [==============================] - 317s 888ms/step - loss: 9.2799e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0814e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-06\n",
      "Epoch 31/50\n",
      "357/357 [==============================] - 311s 871ms/step - loss: 9.4151e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1490e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-06\n",
      "Epoch 32/50\n",
      "357/357 [==============================] - 312s 873ms/step - loss: 9.5024e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1376e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-06\n",
      "Epoch 33/50\n",
      "357/357 [==============================] - 316s 886ms/step - loss: 9.4980e-06 - iou_score: 0.6584 - f1-score: 0.6612 - val_loss: 8.1958e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-06\n",
      "Epoch 34/50\n",
      "357/357 [==============================] - 316s 884ms/step - loss: 9.2149e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1124e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-06\n",
      "Epoch 35/50\n",
      "357/357 [==============================] - 314s 878ms/step - loss: 9.3323e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1347e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-06\n",
      "Epoch 36/50\n",
      "357/357 [==============================] - 315s 883ms/step - loss: 9.2310e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1269e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-06\n",
      "Epoch 37/50\n",
      "357/357 [==============================] - 317s 887ms/step - loss: 9.1993e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1608e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-07\n",
      "Epoch 38/50\n",
      "357/357 [==============================] - 314s 879ms/step - loss: 9.3158e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.0865e-06 - val_iou_score: 0.6648 - val_f1-score: 0.6681 - lr: 3.1623e-07\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 313s 877ms/step - loss: 9.2568e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1512e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-07\n",
      "Epoch 40/50\n",
      "357/357 [==============================] - 320s 895ms/step - loss: 9.0920e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1417e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-07\n",
      "Epoch 41/50\n",
      "357/357 [==============================] - 310s 868ms/step - loss: 9.5051e-06 - iou_score: 0.6584 - f1-score: 0.6612 - val_loss: 8.1317e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-07\n",
      "Epoch 42/50\n",
      "357/357 [==============================] - 318s 890ms/step - loss: 9.2211e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1296e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-07\n",
      "Epoch 43/50\n",
      "357/357 [==============================] - 314s 881ms/step - loss: 9.5243e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1492e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-07\n",
      "Epoch 44/50\n",
      "357/357 [==============================] - 316s 885ms/step - loss: 9.3518e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1091e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 1.0000e-07\n",
      "Epoch 45/50\n",
      "357/357 [==============================] - 312s 875ms/step - loss: 9.4054e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1168e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-07\n",
      "Epoch 46/50\n",
      "357/357 [==============================] - 314s 878ms/step - loss: 9.1783e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1392e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 1.0000e-07\n",
      "Epoch 47/50\n",
      "357/357 [==============================] - 316s 886ms/step - loss: 9.4554e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1310e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-08\n",
      "Epoch 48/50\n",
      "357/357 [==============================] - 314s 879ms/step - loss: 9.1652e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1342e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6628 - lr: 3.1623e-08\n",
      "Epoch 49/50\n",
      "357/357 [==============================] - 313s 878ms/step - loss: 9.1982e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1309e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-08\n",
      "Epoch 50/50\n",
      "357/357 [==============================] - 315s 882ms/step - loss: 9.1017e-06 - iou_score: 0.6574 - f1-score: 0.6602 - val_loss: 8.1443e-06 - val_iou_score: 0.6595 - val_f1-score: 0.6629 - lr: 3.1623e-08\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch=len(train_generator), \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=valid_generator, \n",
    "    validation_steps=len(valid_generator),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "modelPath = './files/SMM_ALLClasses_41.h5'\n",
    "if os.path.exists(modelPath):\n",
    "    raise Exception(\"File already exists\")\n",
    "else:\n",
    "    model.save(modelPath)\n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_df.to_csv(modelPath.replace('.h5','.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator for Unet PermForestChange\n",
    "    \n",
    "    Main parameter is unputTiffs - list to path to tiff files\n",
    "    \n",
    "    Sample usage \n",
    "    allTifs = list(glob.glob('../../ds/tiles_256_256_27/*/*.tif'))\n",
    "\n",
    "    validSize = 250\n",
    "    \n",
    "    train_generator = DataGenerator(allTifs[:-validSize],batch_size=batch)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, inputTiffs, batch_size=32, num_classes=2, shuffle=True):\n",
    "        self.inputTiffs = inputTiffs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        #custom augmentation with imgaug lib\n",
    "        self.aug_pipe = iaa.Sequential(\n",
    "            [\n",
    "                iaa.SomeOf((0,2),\n",
    "                [\n",
    "                   #iaa.Add((-30, -10)), # change brightness of images (by -10 to 10 of original value)\n",
    "                   #iaa.Multiply((0.8, 1.2)),\n",
    "                   \n",
    "                   # Geometric transfroms\n",
    "                    iaa.Rotate((-45,45)),\n",
    "                    iaa.TranslateX(px=(-20, 20)),\n",
    "                    iaa.TranslateY(px=(-20, 20)),\n",
    "                ]\n",
    "                ),\n",
    "            ],\n",
    "           random_order=True\n",
    "       )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputTiffs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        #print (indexes)\n",
    "        batchTifs = [self.inputTiffs[k] for k in indexes]\n",
    "        \n",
    "        X, y = self._get_data(batchTifs)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.inputTiffs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def _get_data(self, batch):\n",
    "        #X = inArray\n",
    "        #y = # logic\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for idx,imagePath in enumerate(batch):\n",
    "            \n",
    "            #print (image)\n",
    "            \n",
    "            # raw image\n",
    "            img = (io.imread(imagePath)/65536).astype('float32')\n",
    "            \n",
    "            # flip with cv2\n",
    "            flip = np.random.binomial(1, .5)\n",
    "            if flip > 0.5:\n",
    "                #img = self.flip(img)\n",
    "                img = self.aug_pipe.augment_image(img)\n",
    "            \n",
    "            # aug with imgaug lib\n",
    "            #img = self.aug_pipe.augment_image(img)\n",
    "            \n",
    "            # x with features\n",
    "            # channel indexes\n",
    "            # mask - 0 \n",
    "            # NEW\n",
    "            #[\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B10\",\"B11\",\"B12\"]\n",
    "            #[ 1,     2,    3,    4,    5,    6,    7,    8,    9,    10,   11,   12,   13  ]\n",
    "            # OLD\n",
    "            #[\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B10\",\"B11\",\"B12\"]\n",
    "            #[ 14,    15,   16,   17,   18,   19,   20,   21,   22,   23,   24,   25,   26  ]\n",
    "            X.append(np.array(\n",
    "                np.dstack([\n",
    "                    #img[:,:,17],# old red\n",
    "                    #img[:,:,21], # new red\n",
    "                    #img[:,:,25], # new red\n",
    "                    #img[:,:,26], # new red\n",
    "                    \n",
    "                    # separate new\n",
    "                    img[:,:,4],\n",
    "                    #img[:,:,5],\n",
    "                    #img[:,:,6],\n",
    "                    #img[:,:,7],\n",
    "                    img[:,:,8],\n",
    "                    #img[:,:,13],\n",
    "                    # separate old\n",
    "                    img[:,:,17],\n",
    "                    #img[:,:,18],\n",
    "                    #img[:,:,19],\n",
    "                    #img[:,:,20],\n",
    "                    img[:,:,21],\n",
    "                    #img[:,:,26],\n",
    "                    #\n",
    "                    img[:,:,4] - img[:,:,17], # dif red\n",
    "                    img[:,:,17] - img[:,:,4], # dif for transitionChange\n",
    "                    #img[:,:,8],\n",
    "                    #img[:,:,21],\n",
    "                    #img[:,:,13],\n",
    "                    img[:,:,8] - img[:,:,21],\n",
    "                    img[:,:,21] - img[:,:,8],\n",
    "                    #(\n",
    "                    #    (img[:,:,8] - img[:,:,4]) /((img[:,:,8] + img[:,:,4])+ 0.0001)- # new ndvi\n",
    "                    #    (img[:,:,21] - img[:,:,17])/((img[:,:,21] + img[:,:,17])+0.0001) # old ndvi\n",
    "                    #), # ndvi diff\n",
    "                    #swvi dif\n",
    "                    #(\n",
    "                    #    (img[:,:,8] - img[:,:,13]) /((img[:,:,8] + img[:,:,13])+0.0001)-   # new swir\n",
    "                    #    (img[:,:,21] - img[:,:,26])/((img[:,:,21] + img[:,:,26])+0.0001)  # old swir\n",
    "                        \n",
    "                    #),\n",
    "                    #swvi dif \n",
    "                    #(\n",
    "                    #    (img[:,:,21] - img[:,:,26])/((img[:,:,21] + img[:,:,26])+0.0001) -\n",
    "                    #    (img[:,:,8] - img[:,:,13]) /((img[:,:,8] + img[:,:,13])+0.0001)   # new swir\n",
    "                         # old swir\n",
    "                        \n",
    "                    #),\n",
    "                    \n",
    "                    img[:,:,13],\n",
    "                    img[:,:,26],\n",
    "                    img[:,:,13] - img[:,:,26],\n",
    "                    img[:,:,26] - img[:,:,13], # dif for transitionChange\n",
    "                    img[:,:,12], # dif for transitionChange\n",
    "                    img[:,:,25], # dif for transitionChange\n",
    "                    img[:,:,12] - img[:,:,25], # dif for transitionChange\n",
    "                    img[:,:,25] - img[:,:,12], # dif for transitionChange\n",
    "                    #(get_value_by_extent(imagePath))/100.0\n",
    "                    \n",
    "                ])).astype('float32'))\n",
    "            \n",
    "            # to one class all changes\n",
    "            y.append(np.where(img[:,:,0]>0,1,0).astype('int'))\n",
    "            \n",
    "        # return x as preprocess With Backbone \n",
    "        #return preprocess_input(np.array(X)), np.array(y)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def flip(self,image):\n",
    "        \"\"\"\n",
    "        Random flip image around x, y or both axes\n",
    "        image is np.array\n",
    "        \"\"\"\n",
    "        flipDirection = np.random.randint(-1,1,1)[0]\n",
    "        return cv2.flip(image,flipDirection)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiClass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom data generator for Unet PermForestChange\n",
    "    \n",
    "    Main parameter is unputTiffs - list to path to tiff files\n",
    "    \n",
    "    Sample usage \n",
    "    allTifs = list(glob.glob('../../ds/tiles_256_256_27/*/*.tif'))\n",
    "\n",
    "    validSize = 250\n",
    "    \n",
    "    train_generator = DataGenerator(allTifs[:-validSize],batch_size=batch)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = {\n",
    "        1: 'Fell',\n",
    "        2: 'Selective_fell',\n",
    "        3: 'Road'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, inputTiffs, batch_size=32, num_classes=2, shuffle=True):\n",
    "        self.inputTiffs = inputTiffs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        #custom augmentation with imgaug lib\n",
    "        self.aug_pipe = iaa.Sequential(\n",
    "            [\n",
    "                iaa.SomeOf((0,2),\n",
    "                [\n",
    "                    #iaa.Add((-30, -10)), # change brightness of images (by -10 to 10 of original value)\n",
    "                    #iaa.Multiply((0.8, 1.2)),\n",
    "                    \n",
    "                    # Geometric transfroms\n",
    "                    iaa.Rotate((-45,45)),\n",
    "                    iaa.TranslateX(px=(-20, 20)),\n",
    "                    iaa.TranslateY(px=(-20, 20)),\n",
    "                ]\n",
    "                ),\n",
    "            ],\n",
    "            random_order=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputTiffs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        #print (indexes)\n",
    "        batchTifs = [self.inputTiffs[k] for k in indexes]\n",
    "        \n",
    "        X, y = self._get_data(batchTifs)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.inputTiffs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def _get_data(self, batch):\n",
    "        #X = inArray\n",
    "        #y = # logic\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for idx,imagePath in enumerate(batch):\n",
    "            \n",
    "            #print (image)\n",
    "            \n",
    "            # raw image\n",
    "            img = io.imread(imagePath)\n",
    "            mask = img[:,:,0]\n",
    "            masks = [(mask == v) for v in self.CLASSES.keys()]\n",
    "            mask = np.stack(masks, axis=-1).astype('int')\n",
    "            # add background if mask is not binary\n",
    "            #if mask.shape[-1] != 1:\n",
    "            #    background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            #    mask = np.concatenate((mask, background), axis=-1)\n",
    "                \n",
    "            # features\n",
    "            img = (img/65536).astype('float32')\n",
    "            \n",
    "            # flip with cv2\n",
    "            #flip = np.random.binomial(1, .5)\n",
    "            #if flip > 0.5:\n",
    "                #img = self.flip(img)\n",
    "            \n",
    "            # aug with imgaug lib\n",
    "            #img = self.aug_pipe.augment_image(img)\n",
    "            \n",
    "            # x with features\n",
    "            # channel indexes\n",
    "            # mask - 0 \n",
    "            # NEW\n",
    "            #[\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B10\",\"B11\",\"B12\"]\n",
    "            #[ 1,     2,    3,    4,    5,    6,    7,    8,    9,    10,   11,   12,   13  ]\n",
    "            # OLD\n",
    "            #[\"B01\",\"B02\",\"B03\",\"B04\",\"B05\",\"B06\",\"B07\",\"B08\",\"B8A\",\"B09\",\"B10\",\"B11\",\"B12\"]\n",
    "            #[ 14,    15,   16,   17,   18,   19,   20,   21,   22,   23,   24,   25,   26  ]\n",
    "            X.append(np.array(\n",
    "                np.dstack([\n",
    "#img[:,:,17],# old red\n",
    "                    #img[:,:,21], # new red\n",
    "                    #img[:,:,25], # new red\n",
    "                    #img[:,:,26], # new red\n",
    "                    \n",
    "                    # separate new\n",
    "                    img[:,:,4],\n",
    "                    #img[:,:,5],\n",
    "                    #img[:,:,6],\n",
    "                    #img[:,:,7],\n",
    "                    img[:,:,8],\n",
    "                    #img[:,:,13],\n",
    "                    # separate old\n",
    "                    img[:,:,17],\n",
    "                    #img[:,:,18],\n",
    "                    #img[:,:,19],\n",
    "                    #img[:,:,20],\n",
    "                    img[:,:,21],\n",
    "                    #img[:,:,26],\n",
    "                    #\n",
    "                    img[:,:,4] - img[:,:,17], # dif red\n",
    "                    img[:,:,17] - img[:,:,4], # dif for transitionChange\n",
    "                    #img[:,:,8],\n",
    "                    #img[:,:,21],\n",
    "                    #img[:,:,13],\n",
    "                    img[:,:,8] - img[:,:,21],\n",
    "                    img[:,:,21] - img[:,:,8],\n",
    "                    #(\n",
    "                    #    (img[:,:,8] - img[:,:,4]) /((img[:,:,8] + img[:,:,4])+ 0.0001)- # new ndvi\n",
    "                    #    (img[:,:,21] - img[:,:,17])/((img[:,:,21] + img[:,:,17])+0.0001) # old ndvi\n",
    "                    #), # ndvi diff\n",
    "                    #swvi dif\n",
    "                    #(\n",
    "                    #    (img[:,:,8] - img[:,:,13]) /((img[:,:,8] + img[:,:,13])+0.0001)-   # new swir\n",
    "                    #    (img[:,:,21] - img[:,:,26])/((img[:,:,21] + img[:,:,26])+0.0001)  # old swir\n",
    "                        \n",
    "                    #),\n",
    "                    #swvi dif \n",
    "                    #(\n",
    "                    #    (img[:,:,21] - img[:,:,26])/((img[:,:,21] + img[:,:,26])+0.0001) -\n",
    "                    #    (img[:,:,8] - img[:,:,13]) /((img[:,:,8] + img[:,:,13])+0.0001)   # new swir\n",
    "                         # old swir\n",
    "                        \n",
    "                    #),\n",
    "                    \n",
    "                    img[:,:,13],\n",
    "                    img[:,:,26],\n",
    "                    img[:,:,13] - img[:,:,26],\n",
    "                    img[:,:,26] - img[:,:,13], # dif for transitionChange\n",
    "                    img[:,:,12], # dif for transitionChange\n",
    "                    img[:,:,25], # dif for transitionChange\n",
    "                    img[:,:,12] - img[:,:,25], # dif for transitionChange\n",
    "                    img[:,:,25] - img[:,:,12], # dif for transitionChange\n",
    "                    #(get_value_by_extent(imagePath))/100.0\n",
    "                    \n",
    "                    \n",
    "                ])).astype('float32'))\n",
    "            \n",
    "            # to one class all changes\n",
    "            #y.append(np.where(img[:,:,0]>0,1,0).astype('int'))\n",
    "            y.append(mask)\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def flip(self,image):\n",
    "        \"\"\"\n",
    "        Random flip image around x, y or both axes\n",
    "        image is np.array\n",
    "        \"\"\"\n",
    "        flipDirection = np.random.randint(-1,1,1)[0]\n",
    "        #return cv2.flip(image,flipDirection)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGen = DataGenerator(list(glob.glob('../../ds/tiles_256_256_27_after_check/*/*with_object.tif')),batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 256, 256, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGen[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "for img in testGen[0][1]:\n",
    "    print(np.unique(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-3e202d17e570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestGen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 3"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(testGen[0][1][0][:,:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
